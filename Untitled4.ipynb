{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a02970b7-faa8-4894-9975-ab9a6ebdeff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV3Large\n",
    "from tensorflow.keras.applications.mobilenet_v3 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, GlobalAveragePooling2D, ReLU, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae16cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============== Data Preparation ==============\n",
    "def prepare_dataset():\n",
    "    base_dir = \"Lumpy Dataset\"\n",
    "    new_base_dir = \"Lumpy Dataset Split\"\n",
    "    original_paths = {\n",
    "        \"Normal\": os.path.join(base_dir, \"NonLumpyCows\", \"ClearImages\"),\n",
    "        \"Stage1\": os.path.join(base_dir, \"LumpyCows\", \"STAGE1\", \"clear visible\"),\n",
    "        \"Severe\": os.path.join(base_dir, \"LumpyCows\", \"SEVER\")\n",
    "    }\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        for class_name in original_paths.keys():\n",
    "            os.makedirs(os.path.join(new_base_dir, split, class_name), exist_ok=True)\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    test_ratio = 0.15\n",
    "    for class_name, original_path in original_paths.items():\n",
    "        images = [f for f in os.listdir(original_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        if not images:\n",
    "            print(f\"Warning: No images found in {original_path}\")\n",
    "            continue\n",
    "        train, test = train_test_split(images, test_size=test_ratio, random_state=42)\n",
    "        train, val = train_test_split(train, test_size=val_ratio / (1 - test_ratio), random_state=42)\n",
    "        def copy_files(files, split_name):\n",
    "            for file in files:\n",
    "                src = os.path.join(original_path, file)\n",
    "                dst = os.path.join(new_base_dir, split_name, class_name, file)\n",
    "                shutil.copy2(src, dst)\n",
    "        copy_files(train, \"train\")\n",
    "        copy_files(val, \"val\")\n",
    "        copy_files(test, \"test\")\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        print(f\"\\n{split.upper()} set:\")\n",
    "        for class_name in original_paths.keys():\n",
    "            path = os.path.join(new_base_dir, split, class_name)\n",
    "            count = len(os.listdir(path)) if os.path.exists(path) else 0\n",
    "            print(f\"{class_name}: {count} images\")\n",
    "    return os.path.join(new_base_dir, \"train\"), os.path.join(new_base_dir, \"val\"), os.path.join(new_base_dir, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00970d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============== Data Generators ==============\n",
    "def get_data_generators(train_dir, val_dir, test_dir, img_size=(224, 224), batch_size=32):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocess_input,\n",
    "        rotation_range=15,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.2,\n",
    "        shear_range=0.1,\n",
    "        brightness_range=[0.8, 1.2],\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    test_val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
    "    val_generator = test_val_datagen.flow_from_directory(\n",
    "        val_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
    "    test_generator = test_val_datagen.flow_from_directory(\n",
    "        test_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical', shuffle=False)\n",
    "    return train_generator, val_generator, test_generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91585e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============== Create Model ==============\n",
    "def create_model(input_shape=(224, 224, 3), num_classes=3, base_trainable=False):\n",
    "    base_model = MobileNetV3Large(include_top=False, input_tensor=Input(shape=input_shape), weights='imagenet')\n",
    "    base_model.trainable = base_trainable\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(256)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(128)(x) #change to 128\n",
    "    x = BatchNormalization()(x) # change\n",
    "    x = ReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    loss_fn = CategoricalCrossentropy(label_smoothing=0.1)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                  loss=loss_fn,\n",
    "                  metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d563e99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============== TTA ==============\n",
    "def tta_predict(model, image_path, tta_steps=5):\n",
    "    from tensorflow.keras.preprocessing import image\n",
    "    img = image.load_img(image_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    preds = []\n",
    "    for _ in range(tta_steps):\n",
    "        aug_img = img_array.copy()\n",
    "        # Optionally, add random flip/rotation here for TTA\n",
    "        aug_img = preprocess_input(aug_img)\n",
    "        aug_img = np.expand_dims(aug_img, axis=0)\n",
    "        pred = model.predict(aug_img, verbose=0)\n",
    "        preds.append(pred[0])\n",
    "    avg_pred = np.mean(preds, axis=0)\n",
    "    return avg_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e79fa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============== Train Model ==============\n",
    "def train_model(train_dir, val_dir, test_dir):\n",
    "    IMG_SHAPE = (224, 224)\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_CLASSES = 3\n",
    "    train_gen, val_gen, test_gen = get_data_generators(train_dir, val_dir, test_dir, IMG_SHAPE, BATCH_SIZE)\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(train_gen.classes), y=train_gen.classes)\n",
    "    class_weights = dict(enumerate(class_weights))\n",
    "    model = create_model(input_shape=IMG_SHAPE + (3,), num_classes=NUM_CLASSES, base_trainable=False)\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6, verbose=1),\n",
    "        ModelCheckpoint(\"lumpy_mobilenetv3large_best.keras\", monitor='val_loss', save_best_only=True)\n",
    "    ]\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=60,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weights\n",
    "    )\n",
    "    # Progressive unfreezing\n",
    "    print(\"\\nUnfreezing base model layers...\")\n",
    "    model.trainable = True\n",
    "    for layer in model.layers[:-20]:\n",
    "        layer.trainable = False\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "                  loss=CategoricalCrossentropy(label_smoothing=0.1),\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(train_gen, validation_data=val_gen, epochs=8, class_weight=class_weights)\n",
    "    # Evaluate\n",
    "    predictions = model.predict(test_gen)\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "    print(\"\\nConfusion Matrix\")\n",
    "    print(confusion_matrix(test_gen.classes, y_pred))\n",
    "    print(\"\\nClassification Report\")\n",
    "    print(classification_report(test_gen.classes, y_pred, target_names=list(test_gen.class_indices.keys())))\n",
    "    model.save(\"lumpy_mobilenetv3large_final.keras\")\n",
    "    plot_training_history(history)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "412c2c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============== Plot History ==============\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"training_history_mobilenetv3large.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "242a0c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============== Predict Single Image ==============\n",
    "def predict_single_image(model_path, image_path, class_names=None, tta_steps=5):\n",
    "    from tensorflow.keras.preprocessing import image\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    avg_pred = tta_predict(model, image_path, tta_steps=tta_steps)\n",
    "    pred_class = np.argmax(avg_pred)\n",
    "    confidence = np.max(avg_pred)\n",
    "    if class_names is None:\n",
    "        class_names = [\"Normal\", \"Stage1\", \"Severe\"]\n",
    "    print(f\"Prediction: {class_names[pred_class]} (confidence: {confidence:.2f})\")\n",
    "    print(f\"Class probabilities: {dict(zip(class_names, avg_pred))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810897cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN set:\n",
      "Normal: 737 images\n",
      "Stage1: 334 images\n",
      "Severe: 439 images\n",
      "\n",
      "VAL set:\n",
      "Normal: 158 images\n",
      "Stage1: 86 images\n",
      "Severe: 142 images\n",
      "\n",
      "TEST set:\n",
      "Normal: 159 images\n",
      "Stage1: 88 images\n",
      "Severe: 125 images\n",
      "Found 1510 images belonging to 3 classes.\n",
      "Found 386 images belonging to 3 classes.\n",
      "Found 372 images belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['keras_tensor']\n",
      "Received: inputs=Tensor(shape=(None, 224, 224, 3))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 700ms/step - accuracy: 0.3297 - loss: 1.4033 - precision: 0.3163 - recall: 0.2364 - val_accuracy: 0.6347 - val_loss: 0.9046 - val_precision: 0.7143 - val_recall: 0.3627 - learning_rate: 1.0000e-04\n",
      "Epoch 2/60\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 602ms/step - accuracy: 0.5046 - loss: 1.1077 - precision: 0.5227 - recall: 0.4055 - val_accuracy: 0.7642 - val_loss: 0.7233 - val_precision: 0.8351 - val_recall: 0.6166 - learning_rate: 1.0000e-04\n",
      "Epoch 3/60\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 921ms/step - accuracy: 0.6345 - loss: 0.9224 - precision: 0.6655 - recall: 0.5619 - val_accuracy: 0.7953 - val_loss: 0.6612 - val_precision: 0.8526 - val_recall: 0.6891 - learning_rate: 1.0000e-04\n",
      "Epoch 4/60\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 770ms/step - accuracy: 0.6850 - loss: 0.8542 - precision: 0.7219 - recall: 0.5981 - val_accuracy: 0.8212 - val_loss: 0.6235 - val_precision: 0.8559 - val_recall: 0.7383 - learning_rate: 1.0000e-04\n",
      "Epoch 5/60\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 806ms/step - accuracy: 0.7196 - loss: 0.7711 - precision: 0.7703 - recall: 0.6625 - val_accuracy: 0.8342 - val_loss: 0.5980 - val_precision: 0.8626 - val_recall: 0.7642 - learning_rate: 1.0000e-04\n",
      "Epoch 6/60\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 866ms/step - accuracy: 0.7446 - loss: 0.7357 - precision: 0.7851 - recall: 0.6967 - val_accuracy: 0.8394 - val_loss: 0.5908 - val_precision: 0.8772 - val_recall: 0.7772 - learning_rate: 1.0000e-04\n",
      "Epoch 7/60\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 844ms/step - accuracy: 0.7753 - loss: 0.7132 - precision: 0.8019 - recall: 0.7182 - val_accuracy: 0.8394 - val_loss: 0.5852 - val_precision: 0.8803 - val_recall: 0.8005 - learning_rate: 1.0000e-04\n",
      "Epoch 8/60\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 736ms/step - accuracy: 0.7773 - loss: 0.7007 - precision: 0.8059 - recall: 0.7383 - val_accuracy: 0.8394 - val_loss: 0.5876 - val_precision: 0.8785 - val_recall: 0.8057 - learning_rate: 1.0000e-04\n",
      "Epoch 9/60\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 735ms/step - accuracy: 0.7975 - loss: 0.6754 - precision: 0.8251 - recall: 0.7506 - val_accuracy: 0.8497 - val_loss: 0.5886 - val_precision: 0.8768 - val_recall: 0.8109 - learning_rate: 1.0000e-04\n",
      "Epoch 10/60\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 661ms/step - accuracy: 0.8297 - loss: 0.6234 - precision: 0.8526 - recall: 0.7881 - val_accuracy: 0.8575 - val_loss: 0.5790 - val_precision: 0.8764 - val_recall: 0.8264 - learning_rate: 1.0000e-04\n",
      "Epoch 11/60\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 468ms/step - accuracy: 0.8087 - loss: 0.6536 - precision: 0.8322 - recall: 0.7626 - val_accuracy: 0.8549 - val_loss: 0.5753 - val_precision: 0.8696 - val_recall: 0.8290 - learning_rate: 1.0000e-04\n",
      "Epoch 12/60\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 416ms/step - accuracy: 0.8129 - loss: 0.6632 - precision: 0.8353 - recall: 0.7725 - val_accuracy: 0.8523 - val_loss: 0.5814 - val_precision: 0.8645 - val_recall: 0.8264 - learning_rate: 1.0000e-04\n",
      "Epoch 13/60\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 451ms/step - accuracy: 0.8222 - loss: 0.6500 - precision: 0.8459 - recall: 0.7843 - val_accuracy: 0.8627 - val_loss: 0.5713 - val_precision: 0.8726 - val_recall: 0.8342 - learning_rate: 1.0000e-04\n",
      "Epoch 14/60\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 490ms/step - accuracy: 0.8316 - loss: 0.6401 - precision: 0.8548 - recall: 0.8037 - val_accuracy: 0.8575 - val_loss: 0.5583 - val_precision: 0.8743 - val_recall: 0.8290 - learning_rate: 1.0000e-04\n",
      "Epoch 15/60\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 453ms/step - accuracy: 0.8175 - loss: 0.6383 - precision: 0.8389 - recall: 0.7793 - val_accuracy: 0.8756 - val_loss: 0.5355 - val_precision: 0.9022 - val_recall: 0.8601 - learning_rate: 1.0000e-04\n",
      "Epoch 16/60\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 482ms/step - accuracy: 0.8382 - loss: 0.6254 - precision: 0.8642 - recall: 0.8026 - val_accuracy: 0.8782 - val_loss: 0.5293 - val_precision: 0.9027 - val_recall: 0.8653 - learning_rate: 1.0000e-04\n",
      "Epoch 17/60\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 461ms/step - accuracy: 0.8233 - loss: 0.6317 - precision: 0.8505 - recall: 0.7998 - val_accuracy: 0.8860 - val_loss: 0.5231 - val_precision: 0.9071 - val_recall: 0.8601 - learning_rate: 1.0000e-04\n",
      "Epoch 18/60\n",
      "\u001b[1m42/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m2s\u001b[0m 372ms/step - accuracy: 0.8523 - loss: 0.5916 - precision: 0.8751 - recall: 0.8274"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============== MAIN ==================\n",
    "if __name__ == \"__main__\":\n",
    "    train_dir, val_dir, test_dir = prepare_dataset()\n",
    "    model, history = train_model(train_dir, val_dir, test_dir)\n",
    "    # Example prediction after training:\n",
    "    # predict_single_image(\"lumpy_mobilenetv3large_best.keras\", \"test_upload.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b39ea1c-0995-457d-bcf3-8a772fc4cb84",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_single_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predict_single_image(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlumpy_mobilenetv3large_model.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLumpy_Skin_14.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m predict_single_image(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlumpy_mobilenetv3large_model.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_upload.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m predict_single_image(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlumpy_mobilenetv3large_model.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_upload1.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict_single_image' is not defined"
     ]
    }
   ],
   "source": [
    "predict_single_image(\"lumpy_mobilenetv3large_model.keras\", \"Lumpy_Skin_14.png\")\n",
    "predict_single_image(\"lumpy_mobilenetv3large_model.keras\", \"test_upload.jpeg\")\n",
    "predict_single_image(\"lumpy_mobilenetv3large_model.keras\", \"test_upload1.jpeg\")\n",
    "predict_single_image(\"lumpy_mobilenetv3large_model.keras\", \"test_upload2.jpeg\")\n",
    "predict_single_image(\"lumpy_mobilenetv3large_model.keras\", \"test_upload3.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d5768a-4d63-4105-bc45-be2a50e6458a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
